# -*- coding: utf-8 -*-
"""finetuned_inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jr91HMolAwdOOf8r9AypQdvFssaxvEN8
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!huggingface-cli login

!pip install datasets
!pip install -U bitsandbytes
!pip install transformers
!pip install peft

import os
import json
import numpy as np
import pandas as pd
import re
import string
from collections import Counter
from tqdm import tqdm
from peft import LoraConfig, get_peft_model
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
from peft import AutoPeftModelForCausalLM, PeftConfig

peft_model_id = "yujinjin/david-gemma-finetuned-4400" ##모델 이름 수정
config = PeftConfig.from_pretrained(peft_model_id)

tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
)
model = AutoPeftModelForCausalLM.from_pretrained(
        peft_model_id,
        device_map="auto",
        quantization_config=bnb_config,
        trust_remote_code=True
)

def remove_newlines(text):
    return text.replace('\n', '')

def remove_double(text):
    return text.replace('  ', '')

def remove_noise(text):
    return text.replace('#', '')

def normalize_answer(s):
    def remove_(text):
        ''' 불필요한 기호 제거 '''
        text = re.sub("'", " ", text)
        text = re.sub('"', " ", text)
        text = re.sub('《', " ", text)
        text = re.sub('》', " ", text)
        text = re.sub('<', " ", text)
        text = re.sub('>', " ", text)
        text = re.sub('〈', " ", text)
        text = re.sub('〉', " ", text)
        text = re.sub("\(", " ", text)
        text = re.sub("\)", " ", text)
        text = re.sub("‘", " ", text)
        text = re.sub("’", " ", text)
        return text

    def white_space_fix(text):
        '''연속된 공백일 경우 하나의 공백으로 대체'''
        return ' '.join(text.split())

    def remove_punc(text):
        '''구두점 제거'''
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        '''소문자 전환'''
        return text.lower()

    return white_space_fix(remove_punc(lower(remove_(s))))

def f1_score(prediction, ground_truth):
    prediction_tokens = normalize_answer(prediction).split()
    ground_truth_tokens = normalize_answer(ground_truth).split()

    # 문자 단위로 f1-score를 계산 합니다.
    prediction_Char = []
    for tok in prediction_tokens:
        now = [a for a in tok]
        prediction_Char.extend(now)

    ground_truth_Char = []
    for tok in ground_truth_tokens:
        now = [a for a in tok]
        ground_truth_Char.extend(now)

    common = Counter(prediction_Char) & Counter(ground_truth_Char)
    num_same = sum(common.values())
    if num_same == 0:
        return 0

    precision = 1.0 * num_same / len(prediction_Char)
    recall = 1.0 * num_same / len(ground_truth_Char)
    f1 = (2 * precision * recall) / (precision + recall)

    return f1

def evaluate(ground_truth_df, predictions_df):
    predictions = dict(zip(predictions_df['question'], predictions_df['answer']))
    f1 = exact_match = total = 0

    for index, row in ground_truth_df.iterrows():
        question_text = row['question']
        ground_truths = row['answer']
        total += 1
        if question_text not in predictions:
            continue
        prediction = predictions[question_text]
        f1 = f1 + f1_score(prediction, ground_truths)

    f1 = 100.0 * f1 / total
    return {'f1': f1}

def generate_response(context, question, model, tokenizer):
    prompt = f"""주어진 문맥을 바탕으로 질문에 대한 답변을 생성하는 챗봇이라고 생각해.
질문에 대한 답변과 그 답변이 올바른 이유인 근거를 모두 제공해야 해.
다음은 그 과정을 수행하는 단계들이야:
1. 문맥에서 질문과 관련된 정보를 찾아줘.
2. 찾은 정보를 기반으로 질문에 대한 답변을 제공해줘.
3. 답변을 제공하는 근거를 문맥에서 찾아 설명해줘.
4. 답변과 근거를 간결하고 명확하게 표현해줘.
5. 답변은 반드시 문맥 안에서만 찾아야해. 기존의 지식은 참고하지 말아줘.

문맥: 강원드림은 일반건축목공반 교육을 국비 지원으로 전액 무료로 진행한다
질문: 춘천시 강원드림에서 진행하는 일반건축목공반 수업의 수강료는 얼마야
근거: 질문에선 수강료를 묻고 있다. 문맥에서 전액 무료로 진행한다고 나와있으므로 답은 전액 무료이다.
답변: 전액 무료

문맥: SNT모티브가 독자 개발한 경찰용 스마트 권총(모델명 STRV-9)을 중동 지역에 공급하는 계약을 맺었다고 19일 밝혔다
질문: 대한민국보다 중동에서 먼저 실사용 되는 권총은 어디서 만들었어
근거: 권총을 만든 회사에 대하 묻고 있다. 문맥에서 'SNT모티브가 독자 개발'이라고 언급되어 있다. 회사의 이름만 물어봤으므로 답은 SNT모티브이다.
답변: SNT모티브

문맥: 약국 등 전문직종과 복권방 등 사행성 업종은 4차 재난지원금 지원 대상에서 제외된다
질문: 4차 재난지원금 지원 대상에서 제외되는 대표적인 사행성 업종은 뭐야
근거: 전문직종의 예시 약국과 사행성 업종의 예시 복권방 중 사행성 업종을 물어봤으므로 답은 복권방이다.
답변: 복권방

문맥: 집 공급을 늘리기 위해 변창흠 국토교통부 장관이 민관 협력 패스트트랙을 꺼내 들었다
질문: 집 공급을 늘리기 위해 변창흠이 내놓은 대책은 뭐야
근거: 변창흠 장관이 민관 협력 패스트트랙을 꺼내 들었다라는 말은 대책을 내놓았다는 말과 같으므로 답은 민관 협력 패스트트랙이다.
답변: 민관 협력 패스트트랙

문맥: 해당사업에는 5년 동안 국비 662억 원과 지방비 165억 원이 투입될 예정이며 산림분야 최초로 목조건축에 3차원 기반설계와 지능형 건설기술이 적용된다.
질문: 국립지덕권산림치유원 사업을 하면서 처음으로 산과 숲에 적용되는 기술은 뭐야
근거: 처음으로 적용된 기술에 대해 묻는 질문이다. 문맥에서 최초로 적용된 기술은 목조건축에 3차원 기반설계와 지능형 건설기술이라고 나와있다.
답변: 목조건축에 3차원 기반설계와 지능형 건설기술

문맥: 30일 한국수력원자력(한수원) 등에 따르면, 울산 울주군 신고리 4호기 원전에서 29일 오전 9시 28분쯤 화재가 발생했다.
질문: 언제 신고리 4호기에서 불길이 일어나는 걸 확인했어
근거: 불길을 확인한 날짜에 대하 묻고있다. 문맥에 29일이라고 명시되어 있으므로 답은 29일이다.
답변: 29일

문맥: {context}
질문: {question}
근거:
답변1:"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=70, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id, temperature=0.3, do_sample=True)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    if "답변1:" in response:
        response = response.split("답변1:", 1)[1][:100]
        if "#" in response:
            response = response.split("#", 1)[0]
        if "답변2:" in response:
            response = response.split("답변2", 1)[0]
        if "근거" in response:
            response = response.split("근거", 1)[0]
        if "\n" in response:
            response = response.replace("\n", "")
        if "-" in response:
            response = response.replace("-", "")
        if "문맥" in response:
            response = response.split("문맥", 1)[0]
        if "질문" in response:
            response = response.split("질문", 1)[0]
        if "답변 근거" in response:
            response = response.split("답변 근거", 1)[0]
        if "틀린 답변" in response:
            response = response.split("틀린 답변", 1)[0]
    return response.strip()

def main():
    test_data = pd.read_csv('/content/drive/MyDrive/open/test.csv')
    test_data['context'] = test_data['context'].apply(remove_newlines)
    test_data['context'] = test_data['context'].apply(remove_double)
    test_data['context'] = test_data['context'].apply(remove_noise)

    submission_dict = {}

    for index, row in test_data.iterrows():
        try:
            context = row['context']
            question = row['question']
            id = row['id']

            if context is not None and question is not None:
                answer = generate_response(context, question, model, tokenizer)
                submission_dict[id] = answer

                # 질문과답변 출력
                print(f"id: {id}")
                print(f"질문: {question}")
                print(f"답변: {answer}")

                print("-" * 50)
            else:
                submission_dict[id] = 'Invalid question or context'
                print(f"Invalid question or context for id: {id}")
                print("-" * 50)

        except Exception as e:
            print(f"Error processing question {id}: {e}")
            print("-" * 50)

    df = pd.DataFrame(list(submission_dict.items()), columns=['id', 'answer'])
    df.to_csv( './ko_gemma_prompt_ver1_submission.csv', index=False)

main()

